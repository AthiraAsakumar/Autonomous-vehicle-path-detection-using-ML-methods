{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install l5kit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q8zqjIeIyxG",
        "outputId": "c32fa929-e9f4-46de-ba92-1b3102fb5b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting l5kit\n",
            "  Using cached l5kit-1.5.0-py3-none-any.whl (156 kB)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from l5kit) (2.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from l5kit) (3.7.1)\n",
            "Collecting numpy~=1.19.0 (from l5kit)\n",
            "  Using cached numpy-1.19.5.zip (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencv-contrib-python-headless (from l5kit)\n",
            "  Obtaining dependency information for opencv-contrib-python-headless from https://files.pythonhosted.org/packages/c6/61/3b6585f22d59fbf11d9bff0d96a6570a8862c00bc0bbb76efc13bc8cef4c/opencv_contrib_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading opencv_contrib_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from l5kit) (3.20.3)\n",
            "Collecting pymap3d (from l5kit)\n",
            "  Using cached pymap3d-3.0.1-py3-none-any.whl (55 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from l5kit) (1.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from l5kit) (67.7.2)\n",
            "Collecting torch<2.0.0,>=1.5.0 (from l5kit)\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Requirement already satisfied: torchvision<1.0.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from l5kit) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from l5kit) (4.65.0)\n",
            "Collecting transforms3d (from l5kit)\n",
            "  Using cached transforms3d-0.4.1.tar.gz (1.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zarr (from l5kit)\n",
            "  Obtaining dependency information for zarr from https://files.pythonhosted.org/packages/57/8e/b74c2a80c7df474c300bab3b761c35d6cfa6468d6d23f891d518ec7c828e/zarr-2.16.0-py3-none-any.whl.metadata\n",
            "  Using cached zarr-2.16.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from l5kit) (6.0.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from l5kit) (6.4.8)\n",
            "Collecting ptable (from l5kit)\n",
            "  Using cached PTable-0.9.2.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from l5kit) (7.7.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from l5kit) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from l5kit) (4.7.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from l5kit) (2.4.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from l5kit) (0.25.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.5.0->l5kit)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.5.0->l5kit)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.5.0->l5kit)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.5.0->l5kit)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.5.0->l5kit) (0.40.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision<1.0.0,>=0.6.0->l5kit) (2.27.1)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision<1.0.0,>=0.6.0 (from l5kit)\n",
            "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "  Using cached torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "  Using cached torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<1.0.0,>=0.6.0->l5kit) (8.4.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->l5kit) (3.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->l5kit) (23.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->l5kit) (6.3.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->l5kit) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->l5kit) (0.0.8)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->l5kit) (3.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (1.4.4)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting matplotlib (from l5kit)\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "  Using cached matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "  Using cached matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->l5kit) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (21.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (5.3.1)\n",
            "Requirement already satisfied: jupyter-client>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (6.1.12)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (5.9.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->l5kit) (0.17.1)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python-headless (from l5kit)\n",
            "  Using cached opencv_contrib_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.3 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.3 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.5 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.9 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.4.60-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (53.8 MB)\n",
            "INFO: pip is still looking at multiple versions of opencv-contrib-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached opencv-contrib-python-headless-4.5.3.56.tar.gz (150.2 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_AiGDjVOnaS",
        "outputId": "9ef8c6c9-5ac9-47b9-c086-446c569db649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   27G   81G  25% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.1G  887M  55% /usr/sbin/docker-init\n",
            "tmpfs           6.4G   16M  6.4G   1% /var/colab\n",
            "/dev/sda1        70G   55G   16G  78% /etc/hosts\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/woven-planet/l5kit.git\n",
        "%cd l5kit/l5kit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIzy5R5xKi4x",
        "outputId": "8c98fe9c-988d-4e67-8e40-47d4de6d9077"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'l5kit' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'l5kit/l5kit'\n",
            "/content/l5kit/l5kit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipenv sync --dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIEtLAsVKxY7",
        "outputId": "78179d41-137b-4d6d-f7a8-2af94b904d4d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: pipenv: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG4BFVFoMLfF",
        "outputId": "4dd05b9a-80da-4d8a-8e83-b8060df57d10"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\"[dev]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ-s1AibK2JE",
        "outputId": "29804a4c-d76b-4943-d9ab-18a7ac982704"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/l5kit/l5kit\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (2.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (3.7.1)\n",
            "Collecting numpy~=1.19.0 (from l5kit==1.5.0)\n",
            "  Using cached numpy-1.19.5.zip (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencv-contrib-python-headless<4.8.0 (from l5kit==1.5.0)\n",
            "  Using cached opencv_contrib_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.3 MB)\n",
            "Requirement already satisfied: protobuf<=3.20,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (3.12.2)\n",
            "Requirement already satisfied: pymap3d in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (2.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (1.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (67.7.2)\n",
            "Collecting torch<2.0.0,>=1.5.0 (from l5kit==1.5.0)\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Requirement already satisfied: torchvision<1.0.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (4.65.0)\n",
            "Requirement already satisfied: transforms3d in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (0.4.1)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (2.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (6.0.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (6.4.8)\n",
            "Requirement already satisfied: ptable in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (0.9.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (7.7.1)\n",
            "Collecting shapely<2.0.0 (from l5kit==1.5.0)\n",
            "  Using cached Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (4.7.1)\n",
            "Requirement already satisfied: bokeh<3.0.0 in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (2.4.3)\n",
            "Collecting importlib-metadata<5.0.0,>=4.10.0 (from l5kit==1.5.0)\n",
            "  Using cached importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Collecting gym==0.22.0 (from l5kit==1.5.0)\n",
            "  Using cached gym-0.22.0.tar.gz (631 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typed-ast==1.5.4 (from l5kit==1.5.0)\n",
            "  Using cached typed_ast-1.5.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (877 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (7.2.2)\n",
            "Collecting mypy (from l5kit==1.5.0)\n",
            "  Obtaining dependency information for mypy from https://files.pythonhosted.org/packages/04/5c/deeac94fcccd11aa621e6b350df333e1b809b11443774ea67582cc0205da/mypy-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached mypy-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting types-PyYAML (from l5kit==1.5.0)\n",
            "  Obtaining dependency information for types-PyYAML from https://files.pythonhosted.org/packages/da/14/7ee3c82b073aa56ba51a7c61e1c37045171fde3d7e60a6e2b1763bdb455c/types_PyYAML-6.0.12.11-py3-none-any.whl.metadata\n",
            "  Using cached types_PyYAML-6.0.12.11-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting twine (from l5kit==1.5.0)\n",
            "  Using cached twine-4.0.2-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (0.40.0)\n",
            "Collecting pytest-cov (from l5kit==1.5.0)\n",
            "  Obtaining dependency information for pytest-cov from https://files.pythonhosted.org/packages/a7/4b/8b78d126e275efa2379b1c2e09dc52cf70df16fc3b90613ef82531499d73/pytest_cov-4.1.0-py3-none-any.whl.metadata\n",
            "  Using cached pytest_cov-4.1.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting flake8 (from l5kit==1.5.0)\n",
            "  Using cached flake8-6.0.0-py2.py3-none-any.whl (57 kB)\n",
            "Collecting isort (from l5kit==1.5.0)\n",
            "  Using cached isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "Requirement already satisfied: Sphinx in /usr/local/lib/python3.10/dist-packages (from l5kit==1.5.0) (3.5.4)\n",
            "Collecting recommonmark (from l5kit==1.5.0)\n",
            "  Using cached recommonmark-0.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pre-commit (from l5kit==1.5.0)\n",
            "  Obtaining dependency information for pre-commit from https://files.pythonhosted.org/packages/e3/b7/1d145c985d8be9729672a45b8b8113030ad60dff45dec592efc4e5f5897a/pre_commit-3.3.3-py2.py3-none-any.whl.metadata\n",
            "  Using cached pre_commit-3.3.3-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting sphinx-press-theme (from l5kit==1.5.0)\n",
            "  Using cached sphinx_press_theme-0.8.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22.0->l5kit==1.5.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.22.0->l5kit==1.5.0) (0.0.8)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh<3.0.0->l5kit==1.5.0) (3.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh<3.0.0->l5kit==1.5.0) (23.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh<3.0.0->l5kit==1.5.0) (8.4.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh<3.0.0->l5kit==1.5.0) (6.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5.0.0,>=4.10.0->l5kit==1.5.0) (3.16.2)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python-headless<4.8.0 (from l5kit==1.5.0)\n",
            "  Using cached opencv_contrib_python_headless-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55.3 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.5 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.9 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.4.60-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
            "  Using cached opencv_contrib_python_headless-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (53.8 MB)\n",
            "  Using cached opencv-contrib-python-headless-4.5.3.56.tar.gz (150.2 MB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zarr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_NiKVwdPLqH",
        "outputId": "259129ad-b26e-495b-e985-d47e6e140f9d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from zarr) (1.22.4)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr) (0.18)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr) (0.11.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from numcodecs>=0.10.0->zarr) (0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# l5kit dependencies\n",
        "!pip install pymap3d==2.1.0\n",
        "!pip install protobuf==3.12.2\n",
        "\n",
        "!pip install ptable\n",
        "\n",
        "!pip uninstall -y typing\n",
        "\n",
        "# The modified l5kit from my github repo (gpu branch)\n",
        "!pip install --no-dependencies git+https://github.com/pestipeti/lyft-l5kit.git@gpu#subdirectory=l5kit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kwLUO6WY9Jc",
        "outputId": "32efa9e3-ced6-466a-956c-b217c6133699"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymap3d==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: protobuf==3.12.2 in /usr/local/lib/python3.10/dist-packages (3.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from protobuf==3.12.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from protobuf==3.12.2) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ptable in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping typing as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pestipeti/lyft-l5kit.git@gpu#subdirectory=l5kit\n",
            "  Cloning https://github.com/pestipeti/lyft-l5kit.git (to revision gpu) to /tmp/pip-req-build-60v04ro2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pestipeti/lyft-l5kit.git /tmp/pip-req-build-60v04ro2\n",
            "  Running command git checkout -b gpu --track origin/gpu\n",
            "  Switched to a new branch 'gpu'\n",
            "  Branch 'gpu' set up to track remote branch 'gpu' from 'origin'.\n",
            "  Resolved https://github.com/pestipeti/lyft-l5kit.git to commit 7fc21b04839289794327cb27f21067391e16723e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymap3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4W1-ll0PdeY",
        "outputId": "78878ae5-01c3-46f6-9a91-209fa73815f1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymap3d in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transforms3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cowK9fIlPrCe",
        "outputId": "077ff836-31aa-4942-bbbf-38ff538aa6b6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transforms3d in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as mpt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from tqdm import tqdm\n",
        "\n",
        "import l5kit\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as mpt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.378579,
          "end_time": "2020-11-10T08:11:03.403729",
          "exception": false,
          "start_time": "2020-11-10T08:10:58.02515",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:23:27.772222Z",
          "iopub.execute_input": "2023-07-18T07:23:27.772595Z",
          "iopub.status.idle": "2023-07-18T07:23:33.403553Z",
          "shell.execute_reply.started": "2023-07-18T07:23:27.772561Z",
          "shell.execute_reply": "2023-07-18T07:23:33.402384Z"
        },
        "trusted": true,
        "id": "OnQ9T9s66dt1"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UH_vMPgJHoH",
        "outputId": "1818cecb-d1d3-4f39-8e65-e2ccf9764ca7"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l5kit.__version__"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.052707,
          "end_time": "2020-11-10T08:11:03.492492",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.439785",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:23:53.009117Z",
          "iopub.execute_input": "2023-07-18T07:23:53.009487Z",
          "iopub.status.idle": "2023-07-18T07:23:53.016422Z",
          "shell.execute_reply.started": "2023-07-18T07:23:53.009454Z",
          "shell.execute_reply": "2023-07-18T07:23:53.015567Z"
        },
        "trusted": true,
        "id": "PC2FFMaq6dt3",
        "outputId": "6dd1eb1a-b5df-4aa0-f659-43b4861550f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setting_seed_value(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "setting_seed_value(42)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.055303,
          "end_time": "2020-11-10T08:11:03.584794",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.529491",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:23:56.788441Z",
          "iopub.execute_input": "2023-07-18T07:23:56.788789Z",
          "iopub.status.idle": "2023-07-18T07:23:56.798374Z",
          "shell.execute_reply.started": "2023-07-18T07:23:56.788758Z",
          "shell.execute_reply": "2023-07-18T07:23:56.797277Z"
        },
        "trusted": true,
        "id": "gqfRZqyH6dt4"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.035005,
          "end_time": "2020-11-10T08:11:03.65662",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.621615",
          "status": "completed"
        },
        "tags": [],
        "id": "Wj-s02mn6dt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Lyft configs ---\n",
        "configuration = {\n",
        "    'format_version': 4,\n",
        "    'data_path': \"/content/drive/MyDrive/lyft-motion-prediction-autonomous-vehicles\",\n",
        "    'model_params': {\n",
        "        'model_architecture': 'resnet34',\n",
        "        'history_num_frames': 10,\n",
        "        'history_step_size': 1,\n",
        "        'history_delta_time': 0.5,\n",
        "        'future_num_frames': 50,\n",
        "        'future_step_size': 1,\n",
        "        'future_delta_time': 0.5,\n",
        "        'model_name': \"model_resnet34_output\",\n",
        "        'lr': 1e-3,\n",
        "        'weight_path': \"/content/drive/MyDrive/lyft_pretrained_model/model_multi_update_lyft_public.pth\",\n",
        "        'train': True,\n",
        "        'predict': True,\n",
        "        \"render_ego_history\": True,\n",
        "        \"step_time\": 0.1\n",
        "    },\n",
        "\n",
        "    'raster_params': {\n",
        "        'raster_size': [224, 224],\n",
        "        'pixel_size': [0.5, 0.5],\n",
        "        'ego_center': [0.25, 0.5],\n",
        "        'map_type': 'py_semantic',\n",
        "        'satellite_map_key': '/content/drive/MyDrive/lyft-motion-prediction-autonomous-vehicles/aerial_map/aerial_map.png',\n",
        "        'semantic_map_key': '/content/drive/MyDrive/lyft-motion-prediction-autonomous-vehicles/semantic_map/semantic_map.pb',\n",
        "        'dataset_meta_key': 'meta.json',\n",
        "        'filter_agents_threshold': 0.5,\n",
        "        \"set_origin_to_bottom\": True,\n",
        "        \"disable_traffic_light_faces\": True\n",
        "    },\n",
        "\n",
        "    'Training_data_loader': {\n",
        "        'key': '/content/drive/MyDrive/lyft-motion-prediction-autonomous-vehicles/scenes/train.zarr',\n",
        "        'batch_size': 16,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 4\n",
        "    },\n",
        "\n",
        "    'Testing_data_loader': {\n",
        "        'key': '/content/drive/MyDrive/lyft-motion-prediction-autonomous-vehicles/scenes/test.zarr',\n",
        "        'batch_size': 32,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 4\n",
        "    },\n",
        "\n",
        "    'Training_params': {\n",
        "        'max_num_steps': 101,\n",
        "        'checkpoint_every_n_steps': 20,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.077609,
          "end_time": "2020-11-10T08:11:03.771515",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.693906",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:24:05.304145Z",
          "iopub.execute_input": "2023-07-18T07:24:05.304501Z",
          "iopub.status.idle": "2023-07-18T07:24:05.315005Z",
          "shell.execute_reply.started": "2023-07-18T07:24:05.304465Z",
          "shell.execute_reply": "2023-07-18T07:24:05.313800Z"
        },
        "trusted": true,
        "id": "l2A-sZUW6dt6"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the train and test data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.036428,
          "end_time": "2020-11-10T08:11:03.915457",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.879029",
          "status": "completed"
        },
        "tags": [],
        "id": "VAtuhuj-6dt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set env variable for data\n",
        "Input_Direct = configuration[\"data_path\"]\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = Input_Direct\n",
        "datamanager = LocalDataManager(None)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.044688,
          "end_time": "2020-11-10T08:11:03.994191",
          "exception": false,
          "start_time": "2020-11-10T08:11:03.949503",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:28:12.547585Z",
          "iopub.execute_input": "2023-07-18T07:28:12.547939Z",
          "iopub.status.idle": "2023-07-18T07:28:12.553277Z",
          "shell.execute_reply.started": "2023-07-18T07:28:12.547904Z",
          "shell.execute_reply": "2023-07-18T07:28:12.551971Z"
        },
        "trusted": true,
        "id": "yy-YIBcF6dt6"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INIT TRAIN DATASET============================================================\n",
        "Training_configuration = configuration[\"Training_data_loader\"]\n",
        "rasterizer_bulding = build_rasterizer(configuration, datamanager)\n",
        "Training_zarr = ChunkedDataset(datamanager.require(Training_configuration[\"key\"])).open()\n",
        "Training_dataset = AgentDataset(configuration, Training_zarr, rasterizer_bulding)\n",
        "Training_dataloader = DataLoader(Training_dataset, shuffle=Training_configuration[\"shuffle\"], batch_size=Training_configuration[\"batch_size\"],\n",
        "                             num_workers=Training_configuration[\"num_workers\"])\n",
        "print(\"==================================TRAIN DATA==================================\")\n",
        "print(Training_dataset)"
      ],
      "metadata": {
        "papermill": {
          "duration": 88.061241,
          "end_time": "2020-11-10T08:12:32.080152",
          "exception": false,
          "start_time": "2020-11-10T08:11:04.018911",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:28:15.845117Z",
          "iopub.execute_input": "2023-07-18T07:28:15.845480Z",
          "iopub.status.idle": "2023-07-18T07:35:21.314238Z",
          "shell.execute_reply.started": "2023-07-18T07:28:15.845445Z",
          "shell.execute_reply": "2023-07-18T07:35:21.313332Z"
        },
        "trusted": true,
        "id": "XSmwTxYU6dt7",
        "outputId": "403ba0b6-b775-43fd-e637-382596afe570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================TRAIN DATA==================================\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#====== INIT TEST DATASET=============================================================\n",
        "Testing_configuration = configuration[\"Testing_data_loader\"]\n",
        "rasterizer_bulding = build_rasterizer(configuration, datamanager)\n",
        "Testing_zarr = ChunkedDataset(datamanager.require(Testing_configuration[\"key\"])).open()\n",
        "Testing_mask = np.load(f\"{Input_Direct}/scenes/mask.npz\")[\"arr_0\"]\n",
        "Testing_dataset = AgentDataset(configuration, Testing_zarr, rasterizer_bulding, agents_mask=Testing_mask)\n",
        "Testing_dataloader = DataLoader(Testing_dataset,shuffle=Testing_configuration[\"shuffle\"],batch_size=Testing_configuration[\"batch_size\"],\n",
        "                             num_workers=Testing_configuration[\"num_workers\"])\n",
        "print(\"==================================TEST DATA==================================\")\n",
        "print(Testing_dataset)"
      ],
      "metadata": {
        "papermill": {
          "duration": 8.913412,
          "end_time": "2020-11-10T08:12:41.019888",
          "exception": false,
          "start_time": "2020-11-10T08:12:32.106476",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:36:37.472259Z",
          "iopub.execute_input": "2023-07-18T07:36:37.472633Z",
          "iopub.status.idle": "2023-07-18T07:36:56.698448Z",
          "shell.execute_reply.started": "2023-07-18T07:36:37.472597Z",
          "shell.execute_reply": "2023-07-18T07:36:56.697481Z"
        },
        "trusted": true,
        "id": "TS3T32R_6dt7",
        "outputId": "52be3642-0719-4485-808d-329f296d0fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================TEST DATA==================================\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|   11314    |  1131400   |  88594921  |    7854144    |      31.43      |        100.00        |        78.31         |        10.00         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple visualization"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.036434,
          "end_time": "2020-11-10T08:12:41.092005",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.055571",
          "status": "completed"
        },
        "tags": [],
        "id": "M6q1X8Ur6dt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us visualize how an input to the model looks like."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.035088,
          "end_time": "2020-11-10T08:12:41.162098",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.12701",
          "status": "completed"
        },
        "tags": [],
        "id": "LODF6tJI6dt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_trajectory(dataset, index, title=\"target_positions movement with draw_trajectory\"):\n",
        "    data = dataset[index]\n",
        "    im = data[\"image\"].transpose(1, 2, 0)\n",
        "    im = dataset.rasterizer.to_rgb(im)\n",
        "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
        "    draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, radius=1, yaws=data[\"target_yaws\"])\n",
        "\n",
        "    mpt.title(title)\n",
        "    mpt.imshow(im[::-1])\n",
        "    mpt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.076794,
          "end_time": "2020-11-10T08:12:41.274713",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.197919",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:05.156923Z",
          "iopub.execute_input": "2023-07-18T07:37:05.157289Z",
          "iopub.status.idle": "2023-07-18T07:37:05.165438Z",
          "shell.execute_reply.started": "2023-07-18T07:37:05.157253Z",
          "shell.execute_reply": "2023-07-18T07:37:05.164251Z"
        },
        "trusted": true,
        "id": "hNvDtajn6dt7"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpt.figure(figsize = (8,6))\n",
        "visualize_trajectory(Training_dataset, index=90)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.437011,
          "end_time": "2020-11-10T08:12:41.738772",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.301761",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:09.633589Z",
          "iopub.execute_input": "2023-07-18T07:37:09.633937Z",
          "iopub.status.idle": "2023-07-18T07:37:10.030969Z",
          "shell.execute_reply.started": "2023-07-18T07:37:09.633902Z",
          "shell.execute_reply": "2023-07-18T07:37:10.030073Z"
        },
        "trusted": true,
        "id": "93xnIG3d6dt8"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.02865,
          "end_time": "2020-11-10T08:12:41.796407",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.767757",
          "status": "completed"
        },
        "tags": [],
        "id": "xhubnEAR6dt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_batch(\n",
        "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute a negative log-likelihood for the multi-modal scenario.\n",
        "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
        "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
        "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
        "    https://leimao.github.io/blog/LogSumExp/\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
        "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
        "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
        "\n",
        "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
        "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
        "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
        "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
        "    # assert all data are valid\n",
        "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
        "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
        "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
        "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
        "\n",
        "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
        "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
        "    avails = avails[:, None, :, None]  # add modes and cords\n",
        "\n",
        "    # error (batch_size, num_modes, future_len)\n",
        "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
        "\n",
        "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
        "        # error (batch_size, num_modes)\n",
        "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
        "\n",
        "    # use max aggregator on modes for numerical stability\n",
        "    # error (batch_size, num_modes)\n",
        "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
        "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
        "    # print(\"error\", error)\n",
        "    return torch.mean(error)\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_single(\n",
        "    gt: Tensor, pred: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
        "    # create confidence (bs)x(mode=1)\n",
        "    batch_size, future_len, num_coords = pred.shape\n",
        "    confidences = pred.new_ones((batch_size, 1))\n",
        "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.055042,
          "end_time": "2020-11-10T08:12:41.938754",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.883712",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:21.067184Z",
          "iopub.execute_input": "2023-07-18T07:37:21.067572Z",
          "iopub.status.idle": "2023-07-18T07:37:21.086682Z",
          "shell.execute_reply.started": "2023-07-18T07:37:21.067535Z",
          "shell.execute_reply": "2023-07-18T07:37:21.085682Z"
        },
        "trusted": true,
        "id": "8PS0knnv6dt8"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.028331,
          "end_time": "2020-11-10T08:12:41.997547",
          "exception": false,
          "start_time": "2020-11-10T08:12:41.969216",
          "status": "completed"
        },
        "tags": [],
        "id": "lwsCGmf76dt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define the baseline model. Note that this model will return three possible trajectories together with confidence score for each trajectory."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.028434,
          "end_time": "2020-11-10T08:12:42.055478",
          "exception": false,
          "start_time": "2020-11-10T08:12:42.027044",
          "status": "completed"
        },
        "tags": [],
        "id": "Smf8DoS16dt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LyftMultiModel(nn.Module):\n",
        "\n",
        "    def __init__(self, configuration: Dict, num_modes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        architecture = configuration[\"model_params\"][\"model_architecture\"]\n",
        "        backbone = eval(architecture)(pretrained=True, progress=True)\n",
        "        self.backbone = backbone\n",
        "\n",
        "        num_history_channels = (configuration[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "        num_in_channels = 3 + num_history_channels\n",
        "\n",
        "        self.backbone.conv1 = nn.Conv2d(\n",
        "            num_in_channels,\n",
        "            self.backbone.conv1.out_channels,\n",
        "            kernel_size=self.backbone.conv1.kernel_size,\n",
        "            stride=self.backbone.conv1.stride,\n",
        "            padding=self.backbone.conv1.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        # This is 512 for resnet18 and resnet34;\n",
        "        # And it is 2048 for the other resnets\n",
        "\n",
        "        if architecture == \"resnet50\":\n",
        "            backbone_out_features = 2048\n",
        "        else:\n",
        "            backbone_out_features = 512\n",
        "\n",
        "        # X, Y coords for the future positions (output shape: batch_sizex50x2)\n",
        "        self.future_len = configuration[\"model_params\"][\"future_num_frames\"]\n",
        "        num_targets = 2 * self.future_len\n",
        "\n",
        "        # You can add more layers here.\n",
        "        self.head = nn.Sequential(\n",
        "            # nn.Dropout(0.2),\n",
        "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
        "        )\n",
        "\n",
        "        self.num_preds = num_targets * num_modes\n",
        "        self.num_modes = num_modes\n",
        "\n",
        "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        x = self.backbone.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.head(x)\n",
        "        x = self.logit(x)\n",
        "\n",
        "        # pred (batch_size)x(modes)x(time)x(2D coords)\n",
        "        # confidences (batch_size)x(modes)\n",
        "        bs, _ = x.shape\n",
        "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
        "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
        "        assert confidences.shape == (bs, self.num_modes)\n",
        "        confidences = torch.softmax(confidences, dim=1)\n",
        "        return pred, confidences"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.055341,
          "end_time": "2020-11-10T08:12:42.140192",
          "exception": false,
          "start_time": "2020-11-10T08:12:42.084851",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:25.737952Z",
          "iopub.execute_input": "2023-07-18T07:37:25.738297Z",
          "iopub.status.idle": "2023-07-18T07:37:25.758833Z",
          "shell.execute_reply.started": "2023-07-18T07:37:25.738261Z",
          "shell.execute_reply": "2023-07-18T07:37:25.757924Z"
        },
        "trusted": true,
        "id": "n_xlsoRU6dt9"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(data, model, device, criterion = pytorch_neg_multi_log_likelihood_batch):\n",
        "    inputs = data[\"image\"].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
        "    targets = data[\"target_positions\"].to(device)\n",
        "    # Forward pass\n",
        "    preds, confidences = model(inputs)\n",
        "    loss = criterion(targets, preds, confidences, target_availabilities)\n",
        "    return loss, preds, confidences"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.040651,
          "end_time": "2020-11-10T08:12:42.209654",
          "exception": false,
          "start_time": "2020-11-10T08:12:42.169003",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:31.640669Z",
          "iopub.execute_input": "2023-07-18T07:37:31.641028Z",
          "iopub.status.idle": "2023-07-18T07:37:31.647620Z",
          "shell.execute_reply.started": "2023-07-18T07:37:31.640995Z",
          "shell.execute_reply": "2023-07-18T07:37:31.646664Z"
        },
        "trusted": true,
        "id": "aXhmNvKk6dt9"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us initialize the model and load the pretrained weights. Note that since the pretrained model was trained on GPU, you also need to enable GPU when running this notebook."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.02869,
          "end_time": "2020-11-10T08:12:42.268319",
          "exception": false,
          "start_time": "2020-11-10T08:12:42.239629",
          "status": "completed"
        },
        "tags": [],
        "id": "F9p6kWJq6dt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== INIT MODEL=================\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LyftMultiModel(configuration)\n",
        "\n",
        "weight_path = configuration[\"model_params\"][\"weight_path\"]\n",
        "if weight_path:\n",
        "    checkpoint = torch.load(weight_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=configuration[\"model_params\"][\"lr\"])\n",
        "print(f'device {device}')"
      ],
      "metadata": {
        "papermill": {
          "duration": 7.548342,
          "end_time": "2020-11-10T08:12:49.845933",
          "exception": false,
          "start_time": "2020-11-10T08:12:42.297591",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:34.562273Z",
          "iopub.execute_input": "2023-07-18T07:37:34.562669Z",
          "iopub.status.idle": "2023-07-18T07:37:40.075027Z",
          "shell.execute_reply.started": "2023-07-18T07:37:34.562634Z",
          "shell.execute_reply": "2023-07-18T07:37:40.074083Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDGgAhaM6dt9",
        "outputId": "d1dc79cb-fb7e-40d0-dcb2-890b909b8d2f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050035,
          "end_time": "2020-11-10T08:12:49.928083",
          "exception": false,
          "start_time": "2020-11-10T08:12:49.878048",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:46.711858Z",
          "iopub.execute_input": "2023-07-18T07:37:46.712200Z",
          "iopub.status.idle": "2023-07-18T07:37:46.719161Z",
          "shell.execute_reply.started": "2023-07-18T07:37:46.712168Z",
          "shell.execute_reply": "2023-07-18T07:37:46.717994Z"
        },
        "trusted": true,
        "id": "UIMbsD_J6dt9",
        "outputId": "dee05955-8a6b-43cb-a7a3-80bd92bbaba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LyftMultiModel(\n",
            "  (backbone): ResNet(\n",
            "    (conv1): Conv2d(25, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "  )\n",
            "  (logit): Linear(in_features=4096, out_features=303, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.031389,
          "end_time": "2020-11-10T08:12:50.0028",
          "exception": false,
          "start_time": "2020-11-10T08:12:49.971411",
          "status": "completed"
        },
        "tags": [],
        "id": "QnM6aZTH6dt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let us implement the training loop, when the **train** parameter is set to True."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033142,
          "end_time": "2020-11-10T08:12:50.067645",
          "exception": false,
          "start_time": "2020-11-10T08:12:50.034503",
          "status": "completed"
        },
        "tags": [],
        "id": "30yZzsEQ6dt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TRAINING LOOP =========================================================\n",
        "if configuration[\"model_params\"][\"train\"]:\n",
        "\n",
        "    tr_it = iter(Training_dataloader)\n",
        "    progress_bar = tqdm(range(configuration[\"Training_params\"][\"max_num_steps\"]))\n",
        "    num_iter = configuration[\"Training_params\"][\"max_num_steps\"]\n",
        "    losses_train = []\n",
        "    iterations = []\n",
        "    metrics = []\n",
        "    times = []\n",
        "    model_name = configuration[\"model_params\"][\"model_name\"]\n",
        "    start = time.time()\n",
        "    for i in progress_bar:\n",
        "        try:\n",
        "            data = next(tr_it)\n",
        "        except StopIteration:\n",
        "            tr_it = iter(Training_dataloader)\n",
        "            data = next(tr_it)\n",
        "        model.train()\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "        loss, _, _ = forward(data, model, device)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses_train.append(loss.item())\n",
        "\n",
        "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
        "        if i % configuration['Training_params']['checkpoint_every_n_steps'] == 0:\n",
        "            torch.save(model.state_dict(), f'{model_name}_{i}.pth')\n",
        "            iterations.append(i)\n",
        "            metrics.append(np.mean(losses_train))\n",
        "            times.append((time.time()-start)/60)\n",
        "\n",
        "    results = pd.DataFrame({'iterations': iterations, 'metrics (avg)': metrics, 'elapsed_time (mins)': times})\n",
        "    results.to_csv(f\"Training_metrics_{model_name}_{num_iter}.csv\", index = False)\n",
        "    print(f\"Total training time is {(time.time()-start)/60} mins\")\n",
        "    print(results.head())"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.052575,
          "end_time": "2020-11-10T08:12:50.15189",
          "exception": false,
          "start_time": "2020-11-10T08:12:50.099315",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:37:54.425653Z",
          "iopub.execute_input": "2023-07-18T07:37:54.426007Z",
          "iopub.status.idle": "2023-07-18T07:37:54.442339Z",
          "shell.execute_reply.started": "2023-07-18T07:37:54.425968Z",
          "shell.execute_reply": "2023-07-18T07:37:54.441595Z"
        },
        "trusted": true,
        "id": "pC5Z4wOw6dt-"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.030875,
          "end_time": "2020-11-10T08:12:50.214327",
          "exception": false,
          "start_time": "2020-11-10T08:12:50.183452",
          "status": "completed"
        },
        "tags": [],
        "id": "x1Cm0qqf6dt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== EVAL LOOP ================================================================\n",
        "if configuration[\"model_params\"][\"predict\"]:\n",
        "\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    # store information for evaluation\n",
        "    future_coords_offsets_pd = []\n",
        "    timestamps = []\n",
        "    confidences_list = []\n",
        "    agent_ids = []\n",
        "\n",
        "    progress_bar = tqdm(Testing_dataloader)\n",
        "    print(progress_bar)\n",
        "\n",
        "    for data in progress_bar:\n",
        "\n",
        "        _, preds, confidences = forward(data, model, device)\n",
        "\n",
        "        #fix for the new environment\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        world_from_agents = data[\"world_from_agent\"].numpy()\n",
        "        centroids = data[\"centroid\"].numpy()\n",
        "        coords_offset = []\n",
        "\n",
        "        # convert into world coordinates and compute offsets\n",
        "        for idx in range(len(preds)):\n",
        "            for mode in range(3):\n",
        "                preds[idx, mode, :, :] = transform_points(preds[idx, mode, :, :], world_from_agents[idx]) - centroids[idx][:2]\n",
        "\n",
        "        future_coords_offsets_pd.append(preds.copy())\n",
        "        # Convert confidences tensor to a NumPy array without gradients\n",
        "        confidences_np = confidences.detach().cpu().numpy()\n",
        "        confidences_list.append(confidences_np.copy())\n",
        "        timestamps.append(data[\"timestamp\"].numpy().copy())\n",
        "        agent_ids.append(data[\"track_id\"].numpy().copy())"
      ],
      "metadata": {
        "papermill": {
          "duration": 6806.850986,
          "end_time": "2020-11-10T10:06:17.159551",
          "exception": false,
          "start_time": "2020-11-10T08:12:50.308565",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T07:38:33.068193Z",
          "iopub.execute_input": "2023-07-18T07:38:33.068563Z",
          "iopub.status.idle": "2023-07-18T09:26:59.812971Z",
          "shell.execute_reply.started": "2023-07-18T07:38:33.068527Z",
          "shell.execute_reply": "2023-07-18T09:26:59.811840Z"
        },
        "trusted": true,
        "id": "-dvO1aMe6dt-"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred_path = 'prediction1.csv'\n",
        "write_pred_csv(pred_path,\n",
        "           timestamps=np.concatenate(timestamps),\n",
        "           track_ids=np.concatenate(agent_ids),\n",
        "           coords=np.concatenate(future_coords_offsets_pd),\n",
        "           confs = np.concatenate(confidences_list)\n",
        "          )"
      ],
      "metadata": {
        "papermill": {
          "duration": 37.17684,
          "end_time": "2020-11-10T10:06:55.101772",
          "exception": false,
          "start_time": "2020-11-10T10:06:17.924932",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-18T09:44:34.430739Z",
          "iopub.execute_input": "2023-07-18T09:44:34.431102Z",
          "iopub.status.idle": "2023-07-18T09:45:10.619852Z",
          "shell.execute_reply.started": "2023-07-18T09:44:34.431067Z",
          "shell.execute_reply": "2023-07-18T09:45:10.618547Z"
        },
        "trusted": true,
        "id": "SGrO93xl6dt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-18T09:47:15.410858Z",
          "iopub.execute_input": "2023-07-18T09:47:15.411226Z",
          "iopub.status.idle": "2023-07-18T09:47:15.415938Z",
          "shell.execute_reply.started": "2023-07-18T09:47:15.411192Z",
          "shell.execute_reply": "2023-07-18T09:47:15.415018Z"
        },
        "trusted": true,
        "id": "jeEci75F6dt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./prediction1.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-18T09:47:18.610502Z",
          "iopub.execute_input": "2023-07-18T09:47:18.610861Z",
          "iopub.status.idle": "2023-07-18T09:47:22.687572Z",
          "shell.execute_reply.started": "2023-07-18T09:47:18.610827Z",
          "shell.execute_reply": "2023-07-18T09:47:22.686724Z"
        },
        "trusted": true,
        "id": "mzJa4dKO6dt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issa1 = df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-18T09:48:53.251407Z",
          "iopub.execute_input": "2023-07-18T09:48:53.251781Z",
          "iopub.status.idle": "2023-07-18T09:48:53.307712Z",
          "shell.execute_reply.started": "2023-07-18T09:48:53.251748Z",
          "shell.execute_reply": "2023-07-18T09:48:53.303821Z"
        },
        "trusted": true,
        "id": "mhZU73zf6dt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}